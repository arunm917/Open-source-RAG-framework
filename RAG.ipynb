{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vJ2C_yB_DsY"
      },
      "source": [
        "# RAG framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88QbKIGdAel8"
      },
      "outputs": [],
      "source": [
        "%pip -q install git+https://github.com/huggingface/transformers # need to install from github\n",
        "%pip install langchain tiktoken chromadb InstructorEmbedding\n",
        "%pip install accelerate loralib bitsandbytes sentencepiece xformers einops\n",
        "%pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8IB_dTEAjtH"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import pipeline\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from IPython.display import Markdown, display\n",
        "import chromadb\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nisc0o9Am8n"
      },
      "outputs": [],
      "source": [
        "# Downloading the LLM\n",
        "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\", torch_dtype=\"auto\", device_map='auto')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\", torch_dtype=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1HKe66oAsfG"
      },
      "outputs": [],
      "source": [
        "corpus_directory = 'Enter path to corpus'\n",
        "loader = DirectoryLoader(corpus_directory, glob=\"*.txt\", loader_cls=TextLoader)\n",
        "documents = loader.load()\n",
        "\n",
        "# Splitting text into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=4096, chunk_overlap=200)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# Get the number of text chunks\n",
        "num_chunks = len(texts)\n",
        "print(f\"Number of text chunks: {num_chunks}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9QI2oQdBI2P"
      },
      "outputs": [],
      "source": [
        "# Download embedding model\n",
        "\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "model_name = \"BAAI/bge-large-en\"\n",
        "encode_kwargs = {'normalize_embeddings': True}  # set True to compute cosine similarity\n",
        "\n",
        "model_norm = HuggingFaceBgeEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs={'device': 'cuda'},\n",
        "    encode_kwargs=encode_kwargs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "So7kgpXHB_c_"
      },
      "outputs": [],
      "source": [
        "persist_directory = 'Specify the path of directory to store the embeddings' # chunksize = 4096\n",
        "\n",
        "# Making a vector database\n",
        "vectordb = Chroma.from_documents(documents=texts,\n",
        "                                embedding=model_norm,\n",
        "                                persist_directory=persist_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jDWQaiMCKIf"
      },
      "outputs": [],
      "source": [
        "# If loading from existing vector database\n",
        "vectordb = Chroma(persist_directory=persist_directory, embedding_function=model_norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vpJwtdoCTAR"
      },
      "outputs": [],
      "source": [
        "# Setup the retriever\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLXjxsnXCXoI"
      },
      "outputs": [],
      "source": [
        "# Functions to process response and return the source\n",
        "import textwrap\n",
        "\n",
        "def wrap_text_preserve_newlines(text, width=110):\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text\n",
        "\n",
        "def process_llm_response(llm_response):\n",
        "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
        "    print('\\n\\nSources:')\n",
        "    for source in llm_response[\"source_documents\"]:\n",
        "        # print(source.metadata['source'])\n",
        "        return(source.metadata['source'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0hp5eeBCdLv"
      },
      "outputs": [],
      "source": [
        "# Create wrokflow for LLM\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length = 5000,\n",
        "    temperature=0.0001,\n",
        "    do_sample=True,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.35\n",
        ")\n",
        "local_llm = HuggingFacePipeline(pipeline = pipe)\n",
        " \n",
        "# Create RAG chain\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=local_llm,\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever=retriever,\n",
        "                                  return_source_documents=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2I8QjN-C0Uf"
      },
      "outputs": [],
      "source": [
        "# Enter query\n",
        "query = \"Input query\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUkh2_SzC8Hi"
      },
      "source": [
        "# Importing question from file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEDxyQnhC-1m"
      },
      "outputs": [],
      "source": [
        "input_file_path = 'Enter path of input text file containing one question per line'\n",
        "output_file_path = 'Enter path of output file to write questions and responses'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90sdY1teDLpf"
      },
      "outputs": [],
      "source": [
        "with open(input_file_path, 'r') as input_file, open(output_file_path, 'w') as output_file:\n",
        "    question_number = 1  # Initialize the question number\n",
        "\n",
        "    # Loop through each line in the input file\n",
        "    for line in input_file:\n",
        "        # Remove leading and trailing whitespaces\n",
        "        question = line.strip()\n",
        "\n",
        "        # Send the question to the qa_chain function to get a response\n",
        "        llm_response = qa_chain(question)\n",
        "        # processed_response = process_llm_response(llm_response)\n",
        "\n",
        "        # Write the question and response to the output file\n",
        "        # output_file.write(f\"Question {question_number}: {question}\\n\")\n",
        "        output_file.write(f\"Question: {question}\\n\")\n",
        "        output_file.write(\"Response:\\n\")\n",
        "        output_file.write(wrap_text_preserve_newlines(llm_response['result']) + '\\n')\n",
        "        output_file.write('\\nSources:\\n')\n",
        "\n",
        "        # Loop through the source documents in the response\n",
        "        for source in llm_response[\"source_documents\"]:\n",
        "            output_file.write(source.metadata['source'] + '\\n')\n",
        "\n",
        "        output_file.write('\\n\\n')  # Separate each question-response pair\n",
        "\n",
        "        question_number += 1  # Increment the question number\n",
        "\n",
        "print(\"Questions and responses written to the output file.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
